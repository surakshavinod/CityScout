{"cells":[{"cell_type":"code","execution_count":3,"id":"5bf347bc","metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyspark in /usr/lib/spark/python (3.3.2)\n","Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/miniconda3/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install pyspark"]},{"cell_type":"code","execution_count":4,"id":"1afadb48","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/11/03 23:57:37 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import when\n","from pyspark.sql import functions as F\n","spark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()"]},{"cell_type":"code","execution_count":5,"id":"44a7bf50","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["incident_table = spark.read.csv('gs://6242filteringbucket/incident_2011_2021NEW.csv', header=True, inferSchema=True)\n","victim_table = spark.read.csv('gs://6242filteringbucket/victim_table_2011_2021.csv', header=True, inferSchema=True)\n","cde_agencies_table = spark.read.csv('gs://6242filteringbucket/cde_agencies_2011_2021_NEW.csv', header=True, inferSchema=True)\n","offense_table = spark.read.csv('gs://6242filteringbucket/offense_2011_2021NEW.csv', header=True, inferSchema=True) #get this table with the data_year\n","offense_type_table = spark.read.csv('gs://6242filteringbucket/offense_type_name_2011_2021NEW.csv', header=True, inferSchema=True)\n","victim_offense_table = spark.read.csv('gs://6242filteringbucket/victim_offense_2011_2021NEW.csv', header=True, inferSchema=True)"]},{"cell_type":"code","execution_count":9,"id":"57bfaab7","metadata":{},"outputs":[],"source":["victim_table = victim_table.select(['incident_id', 'victim_id', 'age_num', 'sex_code', 'race_id', 'ethnicity_id', 'resident_status_code'])\n","offense_table = offense_table.drop('incident_id') #dropping the duplicate incident_id"]},{"cell_type":"code","execution_count":10,"id":"ae379c86","metadata":{},"outputs":[],"source":["incident_victim_df = incident_table.join(victim_table, on='incident_id', how='inner')\n","agencies_incident_victim_df = cde_agencies_table.join(incident_victim_df, on='agency_id', how='inner')\n","offense_victim_offense_df = offense_table.join(victim_offense_table, on='offense_id', how='inner')\n","final_df = agencies_incident_victim_df.join(offense_victim_offense_df, on='victim_id', how='inner')"]},{"cell_type":"code","execution_count":11,"id":"4b131f56","metadata":{},"outputs":[],"source":["\n","\n","# Define the mapping logic using `when` statements for race_id\n","final_df = final_df.withColumn(\n","    'race_id',\n","    F.when(final_df['ethnicity_id'] == 1, 'Hispanic or Latino')\n","     .when(final_df['race_id'] == 0, 'Unknown')\n","     .when(final_df['race_id'] == 1, 'White')\n","     .when(final_df['race_id'] == 2, 'Black or African American')\n","     .when(final_df['race_id'] == 3, 'American Indian or Alaska Native')\n","     .when(final_df['race_id'] == 4, 'Asian')\n","     .when(final_df['race_id'] == 5, 'Asian, Native Hawaiian, or Other Pacific Islander')\n","     .when(final_df['race_id'] == 6, 'Chinese')\n","     .when(final_df['race_id'] == 7, 'Japanese')\n","     .when(final_df['race_id'] == 8, 'Native Hawaiian or Other Pacific Islander')\n","     .when(final_df['race_id'] == 9, 'Other')\n","     .when(final_df['race_id'] == 98, 'Multiple')\n","     .when(final_df['race_id'] == 99, 'Not Specified')\n","     .otherwise(None)\n",")\n","\n","# Filter out the specified race_id categories\n","final_df = final_df.filter(~final_df[\"race_id\"].isin(['Unknown', 'Other', 'Multiple', 'Not Specified']))"]},{"cell_type":"code","execution_count":12,"id":"2a432d6a","metadata":{},"outputs":[],"source":["final_df = final_df.dropDuplicates()"]},{"cell_type":"code","execution_count":13,"id":"3b65738c","metadata":{},"outputs":[],"source":["offense_data = [\n","    (1, 'Justifiable Homicide', 'Homicide Offenses'), \n","    (2, 'False Pretenses/Swindle/Confidence Game', 'Fraud Offenses'), \n","    (3, 'Statutory Rape', 'Sex Offenses'), \n","    (4, 'Sexual Assault With An Object', 'Sex Offenses'), \n","    (5, 'Destruction/Damage/Vandalism of Property', 'Destruction/Damage/Vandalism of Property'), \n","    (6, 'Family Offenses, Nonviolent', None), \n","    (7, 'Theft of Motor Vehicle Parts or Accessories', 'Larceny/Theft Offenses'), \n","    (8, 'Pornography/Obscene Material', 'Pornography/Obscene Material'), \n","    (9, 'Sports Tampering', 'Gambling Offenses'), \n","    (10, 'Driving Under the Influence', None), \n","    (11, 'Counterfeiting/Forgery', 'Counterfeiting/Forgery'), \n","    (12, 'Welfare Fraud', 'Fraud Offenses'), \n","    (13, 'Pocket-picking', 'Larceny/Theft Offenses'), \n","    (14, 'Theft From Motor Vehicle', 'Larceny/Theft Offenses'), \n","    (15, 'Assisting or Promoting Prostitution', 'Prostitution Offenses'), \n","    (16, 'Drug/Narcotic Violations', 'Drug/Narcotic Offenses'), \n","    (17, 'Wire Fraud', 'Fraud Offenses'), \n","    (18, 'Purse-snatching', 'Larceny/Theft Offenses'), \n","    (19, 'Runaway', None), \n","    (20, 'Arson', 'Arson'), \n","    (21, 'Motor Vehicle Theft', 'Motor Vehicle Theft'), \n","    (22, 'Drunkenness', None), \n","    (23, 'Shoplifting', 'Larceny/Theft Offenses'), \n","    (24, 'Operating/Promoting/Assisting Gambling', None), \n","    (25, 'Bad Checks', 'Fraud Offenses'), \n","    (26, 'Extortion/Blackmail', 'Extortion/Blackmail'), \n","    (27, 'Aggravated Assault', 'Assault Offenses'), \n","    (28, 'Stolen Property Offenses', 'Stolen Property Offenses'), \n","    (29, 'Kidnapping/Abduction', 'Kidnapping/Abduction'), \n","    (30, 'Prostitution', 'Prostitution Offenses'), \n","    (31, 'Betting/Wagering', 'Gambling Offenses'), \n","    (32, 'Murder and Nonnegligent Manslaughter', 'Homicide Offenses'), \n","    (33, 'Peeping Tom', None), \n","    (34, 'Trespass of Real Property', None), \n","    (35, 'Drug Equipment Violations', 'Drug/Narcotic Offenses'), \n","    (36, 'Rape', 'Sex Offenses'), \n","    (37, 'Embezzlement', 'Embezzlement'), \n","    (38, 'Negligent Manslaughter', 'Homicide Offenses'), \n","    (39, 'Weapon Law Violations', 'Weapon Law Violations'), \n","    (40, 'Robbery', 'Robbery'), \n","    (41, 'Credit Card/Automated Teller Machine Fraud', 'Fraud Offenses'), \n","    (42, 'Curfew/Loitering/Vagrancy Violations', None), \n","    (43, 'Sodomy', 'Sex Offenses'), \n","    (44, 'Intimidation', 'Assault Offenses'), \n","    (45, 'All Other Larceny', 'Larceny/Theft Offenses'), \n","    (46, 'Impersonation', 'Fraud Offenses'), \n","    (47, 'Theft From Building', 'Larceny/Theft Offenses'), \n","    (48, 'All Other Offenses', None), \n","    (49, 'Burglary/Breaking & Entering', 'Burglary/Breaking & Entering'), \n","    (50, 'Theft From Coin-Operated Machine or Device', 'Larceny/Theft Offenses'), \n","    (51, 'Simple Assault', 'Assault Offenses'), \n","    (52, 'Liquor Law Violations', None), \n","    (53, 'Disorderly Conduct', None), \n","    (54, 'Gambling Equipment Violation', 'Gambling Offenses'), \n","    (55, 'Incest', 'Sex Offenses'), \n","    (56, 'Fondling', 'Sex Offenses'), \n","    (57, 'Bribery', 'Bribery'), \n","    (58, 'Not Specified', None), \n","    (59, 'Human Trafficking, Commercial Sex Acts', 'Human Trafficking'), \n","    (60, 'Human Trafficking, Involuntary Servitude', 'Human Trafficking'), \n","    (61, 'Purchasing Prostitution', 'Prostitution Offenses'), \n","    (62, 'Animal Cruelty', 'Animal Cruelty'), \n","    (63, 'Identity Theft', 'Fraud Offenses'), \n","    (64, 'Hacking/Computer Invasion', 'Fraud Offenses')\n","]\n","\n","offense_data_df = spark.createDataFrame(offense_data, ['offense_type_id', 'offense_name', 'offense_category_name'])\n","\n","# Join final_df with offense_data_df on offense_type_id\n","final_all_df = final_df.join(offense_data_df, on='offense_type_id', how='left') "]},{"cell_type":"code","execution_count":14,"id":"f3c9835d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 32:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+---------------+---------+---------+----------------+--------------+----------+--------+----------+-----------+-------+--------+-------+------------+--------------------+----------+--------------------+---------------------+\n","|offense_type_id|victim_id|agency_id|       city_name|primary_county|state_abbr|state_id|population|incident_id|age_num|sex_code|race_id|ethnicity_id|resident_status_code|offense_id|        offense_name|offense_category_name|\n","+---------------+---------+---------+----------------+--------------+----------+--------+----------+-----------+-------+--------+-------+------------+--------------------+----------+--------------------+---------------------+\n","|             45| 62205773|     8714|Sterling Heights|        Macomb|        MI|      26|    132255|   57347297|     10|       M|  White|           2|                   R|  66872038|   All Other Larceny| Larceny/Theft Off...|\n","|             45| 62206067|    21679|            null|       Raleigh|        WV|      55|     57824|   57347550|     47|       M|  White|           2|                   R|  65928639|   All Other Larceny| Larceny/Theft Off...|\n","|             27| 62206109|    21679|            null|       Raleigh|        WV|      55|     57824|   57347586|     42|       M|  White|           2|                   R|  65911913|  Aggravated Assault|     Assault Offenses|\n","|             21| 62206132|    21679|            null|       Raleigh|        WV|      55|     57824|   57347605|     20|       M|  White|           2|                   R|  65922974| Motor Vehicle Theft|  Motor Vehicle Theft|\n","|             45| 62206137|    21679|            null|       Raleigh|        WV|      55|     57824|   57347610|     25|       F|  White|           2|                   N|  65928650|   All Other Larceny| Larceny/Theft Off...|\n","|              5| 62206160|    21679|            null|       Raleigh|        WV|      55|     57824|   57347632|     46|       M|  White|           2|                   R|  65911922|Destruction/Damag...| Destruction/Damag...|\n","|             49| 62206339|    14791|      N Lawrence|         Stark|        OH|      39|      8270|   57347791|     74|       F|  White|           2|                   R|  63734461|Burglary/Breaking...| Burglary/Breaking...|\n","|             47| 62206339|    14791|      N Lawrence|         Stark|        OH|      39|      8270|   57347791|     74|       F|  White|           2|                   R|  63742381| Theft From Building| Larceny/Theft Off...|\n","|             45| 62206390|    14791|      N Lawrence|         Stark|        OH|      39|      8270|   57347842|     82|       M|  White|           2|                   R|  63742390|   All Other Larceny| Larceny/Theft Off...|\n","|             51| 62206791|     8714|Sterling Heights|        Macomb|        MI|      26|    132255|   57348207|     61|       M|  White|           2|                   R|  66874050|      Simple Assault|     Assault Offenses|\n","+---------------+---------+---------+----------------+--------------+----------+--------+----------+-----------+-------+--------+-------+------------+--------------------+----------+--------------------+---------------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["final_all_df = final_all_df.dropDuplicates()\n","final_all_df.show(10)"]},{"cell_type":"code","execution_count":15,"id":"ba38f98e","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["final_all_df.write.csv('gs://6242filteringbucket/final_all.csv', header=True)"]},{"cell_type":"code","execution_count":16,"id":"0055c226","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/11/04 01:38:05 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Initialize Spark session\n","spark = SparkSession.builder.appName(\"MergeCSV\").getOrCreate()\n"]},{"cell_type":"code","execution_count":28,"id":"cc9c50e6","metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'gs://6242filteringbucket/victim_table_2011_2021.csv'","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)","\u001B[0;32m/tmp/ipykernel_14710/2914984186.py\u001B[0m in \u001B[0;36m<cell line: 9>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;31m# Open the output file in write mode\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"w\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0moutfile\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m     \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;31m#     # Write header from the first file\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'gs://6242filteringbucket/victim_table_2011_2021.csv'"]}],"source":["import pandas as pd\n","\n","# List of file names, change the path if files are in a different directory\n","file_names = [f\"gs://6242filteringbucket/final_all.csv/part-{i:05d}-58716585-8ef7-4182-9bc0-ae3052657caf-c000.csv\" for i in range(67)]\n","\n","output_file = 'gs://6242filteringbucket/merged_output.csv'\n","\n","# Open the output file in write mode\n","with open(output_file, \"w\") as outfile:\n","    x = 1\n","#     # Write header from the first file\n","#     with open(file_names[0], \"r\") as first_file:\n","#         outfile.write(first_file.readline())  # Write the header line once\n","\n","#     # Append the rest of the files\n","#     for file_name in file_names:\n","#         with open(file_name, \"r\") as infile:\n","#             next(infile)  # Skip the header line for each file\n","#             # Write the rest of the lines\n","#             outfile.writelines(infile.readlines())\n"]},{"cell_type":"code","execution_count":29,"id":"a484c1d5","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["\n","# Load the two CSV files\n","df1 = spark.read.csv(\"gs://6242filteringbucket/city_scout_fbi_nibrs_2011_2021.csv\", header=True, inferSchema=True)\n","df2 = spark.read.csv(\"gs://6242filteringbucket/offense_dates.csv\", header=True, inferSchema=True)\n","\n","\n","# Perform the join on offense_id\n","merged_df = df1.join(df2, on=\"offense_id\", how=\"inner\")  # Use \"inner\" for matching rows only; can be \"left\" or \"outer\" as needed\n"]},{"cell_type":"code","execution_count":32,"id":"5d9c012a","metadata":{},"outputs":[],"source":["merged_df = merged_df.dropDuplicates()\n","# Save the merged DataFrame to a new CS\n","# merged_df.write.csv(\"gs://6242filteringbucket/final_csv_with_dates.csv\", header=True)"]},{"cell_type":"code","execution_count":44,"id":"361c0cfd","metadata":{},"outputs":[{"ename":"AnalysisException","evalue":"path gs://6242filteringbucket/final2015_2021.csv already exists.","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)","\u001B[0;32m/tmp/ipykernel_14710/2221805258.py\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# sort merged_df for data year between 2015 and 2020\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mfiltered_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmerged_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmerged_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_year\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0;36m2015\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m&\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mmerged_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_year\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m2021\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mfiltered_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"gs://6242filteringbucket/final2015_2021.csv\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mcsv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   1238\u001B[0m             \u001B[0mlineSep\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlineSep\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1239\u001B[0m         )\n\u001B[0;32m-> 1240\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1241\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1242\u001B[0m     def orc(\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31mAnalysisException\u001B[0m: path gs://6242filteringbucket/final2015_2021.csv already exists."]}],"source":["# sort merged_df for data year between 2015 and 2020\n","filtered_df = merged_df.filter((merged_df.data_year >= 2017) & (merged_df.data_year <= 2021))\n","filtered_df.write.csv(\"gs://6242filteringbucket/final2017_2021.csv\", header=True)\n","                                                                                \n"]},{"cell_type":"code","execution_count":35,"id":"9eb447aa","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# merge the files using a separate python file in the analysis cluster\n","# different strat\n","new_merged_df = spark.read.option(\"header\", \"true\").csv(\"gs://6242filteringbucket/final2017_2021.csv/\")\n","new_merged_single_file_df = new_merged_df.coalesce(1)\n","new_merged_single_file_df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"gs://6242filteringbucket/final2017_2021_merged.csv\")"]},{"cell_type":"code","execution_count":43,"id":"1fc2983a","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# sort merged_df for data year between 2015 and 2020\n","filtered_df = merged_df.filter((merged_df.data_year >= 2015) & (merged_df.data_year <= 2021))\n","filtered_df.write.csv(\"gs://6242filteringbucket/final2015_2021.csv\", header=True)\n","                                                                                \n"]},{"cell_type":"code","execution_count":45,"id":"d5dc6eed","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# merge the files using a separate python file in the analysis cluster\n","# different strat\n","new_merged_df = spark.read.option(\"header\", \"true\").csv(\"gs://6242filteringbucket/final2015_2021.csv/\")\n","new_merged_single_file_df = new_merged_df.coalesce(1)\n","new_merged_single_file_df.write.option(\"header\", \"true\").mode(\"error\").csv(\"gs://6242filteringbucket/final2015_2021_merged.csv\")"]},{"cell_type":"code","execution_count":null,"id":"440ed918","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}